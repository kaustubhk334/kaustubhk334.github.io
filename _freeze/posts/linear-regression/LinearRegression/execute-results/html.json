{
  "hash": "6bbacd995cf5178086defa62fb17b748",
  "result": {
    "markdown": "---\ntitle: What is Linear Regression?\n---\n\nRegression is a model that uses variables to predict the outcomes of other variables. Linear regression assumes the variables have a linear relation, and hence tries to predict outcomes linearly. Some applications of linear regression can include predicting survival rate, political affiliation, or even insurance costs. There are two types of linear regression, simple and multi, which only differ by the number of predictor variables. \n\n\n### Example Dataset ###\nFor this linear regression example we will use a simple salary prediction dataset from Kaggle. The data set correlates years of experience with salary, and we can use simple linear regression to predict salary based on experience. If there were more predictors other than years of experience in the dataset we would use multilinear regression, but the underyling concept is the same - fitting predictors across a line plot to predict a variable\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\ndf = pd.read_csv('Salary_dataset.csv')\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>YearsExperience</th>\n      <th>Salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>30.000000</td>\n      <td>30.000000</td>\n      <td>30.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>14.500000</td>\n      <td>5.413333</td>\n      <td>76004.000000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>8.803408</td>\n      <td>2.837888</td>\n      <td>27414.429785</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.200000</td>\n      <td>37732.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>7.250000</td>\n      <td>3.300000</td>\n      <td>56721.750000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>14.500000</td>\n      <td>4.800000</td>\n      <td>65238.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>21.750000</td>\n      <td>7.800000</td>\n      <td>100545.750000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>29.000000</td>\n      <td>10.600000</td>\n      <td>122392.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Preprocessing ###\nWe need to do some dataset cleaning to easily feed it into the linear regression model and do visualizations. This dataset has an extra column and could have duplicate/null values.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = df.drop('Unnamed: 0', axis=1)\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>YearsExperience</th>\n      <th>Salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>30.000000</td>\n      <td>30.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.413333</td>\n      <td>76004.000000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.837888</td>\n      <td>27414.429785</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.200000</td>\n      <td>37732.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3.300000</td>\n      <td>56721.750000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>4.800000</td>\n      <td>65238.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.800000</td>\n      <td>100545.750000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>10.600000</td>\n      <td>122392.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf.isnull().sum(), df.duplicated().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n(YearsExperience    0\n Salary             0\n dtype: int64,\n 0)\n```\n:::\n:::\n\n\n### Assumptions ###\nThere are a few assumptions before we can properly use linear regression on a dataset. To validate those assumptions exist we need to do some visualization on the dataset.\n\n1. **Residual Independence**: There shouldn't be correlation between residual terms. We can see from the first visualization there is no such correlation.\n2. **Residual Linearity**: We can see that there is a linear relationship between the independent and dependent variable. We can safely use linear regression to assume a line of best fit for prediction.\n3. **Normal Distribution**: While not a perfect normal distribution, the residuals do form close to a normal distribution.\n4. **Equal Variance**: The residuals have close to equal variance and there aren't too many outliers or extreme values.\n\nWe can see from the assumptions listed above that the datset does pretty much conform to these assumptions and we can use linear regression to predict salary based on years of experience. One shortcoming of this dataset is the size, however, it would be nice to have more data points to additionally confirm these assumptions. Nevertheless, we can safely carry out linear regression.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nsns.residplot(df, x='YearsExperience', y='Salary')\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n<Axes: xlabel='YearsExperience', ylabel='Salary'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](LinearRegression_files/figure-html/cell-6-output-2.png){width=613 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nsns.regplot(df, x='YearsExperience', y='Salary')\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n<Axes: xlabel='YearsExperience', ylabel='Salary'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](LinearRegression_files/figure-html/cell-7-output-2.png){width=618 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsns.displot(df, x=\"YearsExperience\", kde=True)\n```\n\n::: {.cell-output .cell-output-display}\n![](LinearRegression_files/figure-html/cell-8-output-1.png){width=470 height=470}\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsns.displot(df, x=\"Salary\", kde=True)\n```\n\n::: {.cell-output .cell-output-display}\n![](LinearRegression_files/figure-html/cell-9-output-1.png){width=470 height=470}\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nsns.heatmap(df.corr(), annot=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](LinearRegression_files/figure-html/cell-10-output-2.png){width=559 height=416}\n:::\n:::\n\n\n### Linear Regression ###\nWe will perform linear regression by first splitting the data into train/test. We will use an 80/20 split for train/test data.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nX=df['YearsExperience']\nY=df['Salary']\nxTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.2)\nlen(xTrain), len(yTrain)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n(24, 24)\n```\n:::\n:::\n\n\nWe will use scikit-learn's built in linear regression model to make our predictions\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nlr = LinearRegression()\nxTrain = np.array(xTrain).reshape(-1, 1)\nyTrain = np.array(yTrain).reshape(-1, 1)\nlr.fit(xTrain, yTrain)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nxTest = np.array(xTest).reshape(-1, 1)\nlr.predict(xTest)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\narray([[125736.3007223 ],\n       [ 75827.51046153],\n       [ 55672.0374716 ],\n       [117098.24086948],\n       [123816.73186612],\n       [ 93103.63016718]])\n```\n:::\n:::\n\n\nWe can see our linear regression line closely resembles our train data, albeit with a little bit of variance, which is to be expected.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nplt.scatter(df['YearsExperience'], df['Salary'])\nplt.plot(xTest, lr.predict(xTest),color='g')\n```\n\n::: {.cell-output .cell-output-display}\n![](LinearRegression_files/figure-html/cell-14-output-1.png){width=600 height=411}\n:::\n:::\n\n\n### Calculating Accuracy ###\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nyPred = lr.predict(xTest)\nrmse = np.sqrt(mean_squared_error(yTest, yPred))\nr2 = r2_score(yTest, yPred)\n\nrmse, r2\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n(4431.5473096452315, 0.9662834154887496)\n```\n:::\n:::\n\n\nWe can see that linear regression can predict closely with about 92.7% accuracy as well as a variance of $7289 with salary amount. Perhaps the results would be stronger with a larger dataset or other factors. However, for the purposes of simple linear regression, this example is sufficient.\n\n",
    "supporting": [
      "LinearRegression_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}