[
  {
    "objectID": "posts/nonlinear-regression/NonlinearRegression.html",
    "href": "posts/nonlinear-regression/NonlinearRegression.html",
    "title": "What is Nonlinear Regression?",
    "section": "",
    "text": "Nonlinear, like linear regression tries to model a relation between independent and dependent variables using a function. However, linear regression follows a traditional y=mx+b line graph, while nonlinear relations can have more complex functions of different orders. Some example functions are quadratic, exponential, logistic, logarithmic, etc. To find the fitness of the nonlinear regression model, a sum of squares is used (calculating the difference of the mean of the data and each point). Use cases for nonlinear regression consist of any time of nonlinear data, such as population growth over time, stock market forecasting, etc.\n\nSome types of Nonlinear Regression\n\n\nQuadratic\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef quadratic(x, a, b, c):\n    return a * x**2 + b * x + c\n\nx = np.linspace(-10, 10, 100)\n\na = 1\nb = 2\nc = 1\n\ny = quadratic(x, a, b, c)\n\nnum_points = 50\nrandX = np.random.uniform(-8, 8, num_points)\nrandY = quadratic(randX, a, b, c) + np.random.normal(0, 5, num_points)\n\nplt.plot(x, y)\nplt.scatter(randX, randY, color='red', label='Random Data Points')\n\nplt.title('Quadratic Function with Scatter Plot')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(0, color='k', linewidth=0.5)\nplt.axvline(0, color='k', linewidth=0.5)\nplt.grid(color='y', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n\nExponential\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef exponential(x, a, b):\n    return a * np.exp(b * x)\n\nx = np.linspace(-10, 10, 100)\n\na = 2\nb = 1\n\ny = exponential(x, a, b)\n\nnum_points = 50\nrandX = np.random.uniform(-8, 8, num_points)\nrandY = exponential(randX, a, b) + np.random.normal(0, 5, num_points)\n\nplt.plot(x, y)\nplt.scatter(randX, randY, color='red', label='Random Data Points')\n\nplt.title('Exponential Function with Scatter Plot')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(0, color='k', linewidth=0.5)\nplt.axvline(0, color='k', linewidth=0.5)\nplt.grid(color='y', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n\nLogistic\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef logistic(x, a, b):\n    return 1 / (1 +np.exp(-a*(x-b)))\n\nx = np.linspace(-10, 10, 100)\n\na = 1\nb = 2\n\ny = logistic(x, a, b)\n\nnum_points = 50\nrandX = np.random.uniform(-8, 8, num_points)\nrandY = logistic(randX, a, b) + np.random.normal(0, 5, num_points)\n\nplt.plot(x, y)\nplt.scatter(randX, randY, color='red', label='Random Data Points')\n\nplt.title('Logistic Function with Scatter Plot')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(0, color='k', linewidth=0.5)\nplt.axvline(0, color='k', linewidth=0.5)\nplt.grid(color='y', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef logarithmic(x, a, b):\n    return a * np.log(x) + b\n\nx = np.linspace(0.1, 10, 100)\n\na = 2\nb = 1\n\ny = logarithmic(x, a, b)\n\nnum_points = 50\nrandX = np.random.uniform(0.1, 8, num_points)\nrandY = logarithmic(randX, a, b) + np.random.normal(0, 5, num_points)\n\nplt.plot(x, y)\nplt.scatter(randX, randY, color='red', label='Random Data Points')\n\nplt.title('Logarithmic Function with Scatter Plot')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(0, color='k', linewidth=0.5)\nplt.axvline(0, color='k', linewidth=0.5)\nplt.grid(color='y', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n\nWorld GDP Dataset\nFor this nonlinear regression analysis we will use a dataset of World GDP over time. The data just shows how World GDP changed every year and we can use some form of nonlinear regression to predict how it will be in the future. Nonlinear regression is great for time series data. This dataset is publicly available.\n\nimport pandas as pd\ndf = pd.read_csv('Worldgdp.csv')\ndf.describe()\n\n\n\n\n\n\n\n\nYear\nGDP\n\n\n\n\ncount\n66.000000\n6.600000e+01\n\n\nmean\n1982.500000\n4.328561e+13\n\n\nstd\n19.196354\n2.776368e+13\n\n\nmin\n1950.000000\n9.250000e+12\n\n\n25%\n1966.250000\n1.987500e+13\n\n\n50%\n1982.500000\n3.630000e+13\n\n\n75%\n1998.750000\n5.970000e+13\n\n\nmax\n2015.000000\n1.080000e+14\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.regplot(df, x='Year', y='GDP')\n\n&lt;Axes: xlabel='Year', ylabel='GDP'&gt;\n\n\n\n\n\nAs we can see, linear regression doesn’t show a good prediction of future GDP so we need to use one of the nonlinear regression functions. Just intuitively from the graph, we can see that a logistic function will probably fit the function closely. We can use scipy to optimize the function.\ndef logistic(x, a, b): return 1 / (1 +np.exp(-a*(x-b)))\nY_pred = logistic(df[‘Year’], 0.1 , 2000) plt.plot(df[‘Year’], Y_pred*1e14) plt.plot(df[‘Year’], df[‘GDP’], ‘g.’)\n\nfrom scipy.optimize import curve_fit\nxVals = df['Year'] / max(df['Year'])\nyVals = df['GDP'] / max(df['GDP'])\n(a, b), _ = curve_fit(logistic, xVals, yVals)\n\n\nx = np.linspace(min(df['Year']), max(df['Year']))\nx /= max(x)\ny = logistic(x, a, b)\nplt.plot(xVals* max(df['Year']), yVals*max(df['GDP']), 'g.')\nplt.plot(x*max(df['Year']),y*max(df['GDP']), linewidth=2.0)\nplt.ylabel('World GDP')\nplt.xlabel('Year')\nplt.show()\n\n\n\n\nWe can intuitively see that scipy managed to fit the logistic function appropriately with our data and it resembles pretty closely\nLets predict world GDP in 2030 using the nonlinear regression model\n\n(logistic([2035/max(df['Year'])], a, b) * 1e14)[0]\n\n94677239359468.88"
  },
  {
    "objectID": "posts/clustering/Clustering.html",
    "href": "posts/clustering/Clustering.html",
    "title": "Clustering ##",
    "section": "",
    "text": "Clustering is grouping together a bunch of data points into smaller subsections or “clusters”. These data points belong to a certain group as they share certain characteristics. An example can be clustering a dataset of people based on age, sex, race, height, etc. Multiple datapoints can belong to different clusters depending on the differentiating formula chosen. There are many different types of clustering like centrality, distribution models, connectivity models, etc. In this post, we will look at K-Means and Hierarchical clustering, both of which are unsupervised clustering techniques. Clustering can have pretty much any application, including sports, voting, or even the stock market. The possibilities are endless."
  },
  {
    "objectID": "posts/clustering/Clustering.html#credit-card-dataset",
    "href": "posts/clustering/Clustering.html#credit-card-dataset",
    "title": "Clustering ##",
    "section": "Credit Card Dataset",
    "text": "Credit Card Dataset\nThis dataset contains the credit card info for many users and has a lot of columns showing the different characteristics. Clustering algorithms can use these characateristics to group togeth the users into “clusters”\n\nimport pandas as pd\ndf = pd.read_csv('cc.csv')\ndf.describe()\n\n\n\n\n\n\n\n\nBALANCE\nBALANCE_FREQUENCY\nPURCHASES\nONEOFF_PURCHASES\nINSTALLMENTS_PURCHASES\nCASH_ADVANCE\nPURCHASES_FREQUENCY\nONEOFF_PURCHASES_FREQUENCY\nPURCHASES_INSTALLMENTS_FREQUENCY\nCASH_ADVANCE_FREQUENCY\nCASH_ADVANCE_TRX\nPURCHASES_TRX\nCREDIT_LIMIT\nPAYMENTS\nMINIMUM_PAYMENTS\nPRC_FULL_PAYMENT\nTENURE\n\n\n\n\ncount\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8949.000000\n8950.000000\n8637.000000\n8950.000000\n8950.000000\n\n\nmean\n1564.474828\n0.877271\n1003.204834\n592.437371\n411.067645\n978.871112\n0.490351\n0.202458\n0.364437\n0.135144\n3.248827\n14.709832\n4494.449450\n1733.143852\n864.206542\n0.153715\n11.517318\n\n\nstd\n2081.531879\n0.236904\n2136.634782\n1659.887917\n904.338115\n2097.163877\n0.401371\n0.298336\n0.397448\n0.200121\n6.824647\n24.857649\n3638.815725\n2895.063757\n2372.446607\n0.292499\n1.338331\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n50.000000\n0.000000\n0.019163\n0.000000\n6.000000\n\n\n25%\n128.281915\n0.888889\n39.635000\n0.000000\n0.000000\n0.000000\n0.083333\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n1600.000000\n383.276166\n169.123707\n0.000000\n12.000000\n\n\n50%\n873.385231\n1.000000\n361.280000\n38.000000\n89.000000\n0.000000\n0.500000\n0.083333\n0.166667\n0.000000\n0.000000\n7.000000\n3000.000000\n856.901546\n312.343947\n0.000000\n12.000000\n\n\n75%\n2054.140036\n1.000000\n1110.130000\n577.405000\n468.637500\n1113.821139\n0.916667\n0.300000\n0.750000\n0.222222\n4.000000\n17.000000\n6500.000000\n1901.134317\n825.485459\n0.142857\n12.000000\n\n\nmax\n19043.138560\n1.000000\n49039.570000\n40761.250000\n22500.000000\n47137.211760\n1.000000\n1.000000\n1.000000\n1.500000\n123.000000\n358.000000\n30000.000000\n50721.483360\n76406.207520\n1.000000\n12.000000"
  },
  {
    "objectID": "posts/clustering/Clustering.html#k-means-clustering",
    "href": "posts/clustering/Clustering.html#k-means-clustering",
    "title": "Clustering ##",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nThis is a supervised learning algorithm used to fit a group of datasets into k clusters. The algorithm provides the centroids of the k clusters, which are found by grouping the datapoints based on shared characteristics. The centroids are calculated using the euclidian distance of two data points. The Elbow Criterion method can be used to optimize k by running K-Means clustering on a bunch of different k values. Intuitvely, the number of data points in a cluster decreases as k increases.\n\ndf = df.drop('CUST_ID', axis=1)\ndf = df.ffill()\ndf.describe()\n\n\n\n\n\n\n\n\nBALANCE\nBALANCE_FREQUENCY\nPURCHASES\nONEOFF_PURCHASES\nINSTALLMENTS_PURCHASES\nCASH_ADVANCE\nPURCHASES_FREQUENCY\nONEOFF_PURCHASES_FREQUENCY\nPURCHASES_INSTALLMENTS_FREQUENCY\nCASH_ADVANCE_FREQUENCY\nCASH_ADVANCE_TRX\nPURCHASES_TRX\nCREDIT_LIMIT\nPAYMENTS\nMINIMUM_PAYMENTS\nPRC_FULL_PAYMENT\nTENURE\n\n\n\n\ncount\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n8950.000000\n\n\nmean\n1564.474828\n0.877271\n1003.204834\n592.437371\n411.067645\n978.871112\n0.490351\n0.202458\n0.364437\n0.135144\n3.248827\n14.709832\n4494.394205\n1733.143852\n865.225790\n0.153715\n11.517318\n\n\nstd\n2081.531879\n0.236904\n2136.634782\n1659.887917\n904.338115\n2097.163877\n0.401371\n0.298336\n0.397448\n0.200121\n6.824647\n24.857649\n3638.616165\n2895.063757\n2376.929826\n0.292499\n1.338331\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n50.000000\n0.000000\n0.019163\n0.000000\n6.000000\n\n\n25%\n128.281915\n0.888889\n39.635000\n0.000000\n0.000000\n0.000000\n0.083333\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n1600.000000\n383.276166\n169.488256\n0.000000\n12.000000\n\n\n50%\n873.385231\n1.000000\n361.280000\n38.000000\n89.000000\n0.000000\n0.500000\n0.083333\n0.166667\n0.000000\n0.000000\n7.000000\n3000.000000\n856.901546\n312.096808\n0.000000\n12.000000\n\n\n75%\n2054.140036\n1.000000\n1110.130000\n577.405000\n468.637500\n1113.821139\n0.916667\n0.300000\n0.750000\n0.222222\n4.000000\n17.000000\n6500.000000\n1901.134317\n815.375602\n0.142857\n12.000000\n\n\nmax\n19043.138560\n1.000000\n49039.570000\n40761.250000\n22500.000000\n47137.211760\n1.000000\n1.000000\n1.000000\n1.500000\n123.000000\n358.000000\n30000.000000\n50721.483360\n76406.207520\n1.000000\n12.000000\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score"
  },
  {
    "objectID": "posts/clustering/Clustering.html#normalize-data",
    "href": "posts/clustering/Clustering.html#normalize-data",
    "title": "Clustering ##",
    "section": "Normalize Data",
    "text": "Normalize Data\nFirst we need to normalize the data so we can feed to the clustering algorithm. We can use PCA (Principle Component Analysis) for this which will reduce the number of dimensions of the dataset so we can run the K-Means model efficiently.\n\nscaler = StandardScaler() \ndf = scaler.fit_transform(df) \n\n\ndf = normalize(df)\npca = PCA(n_components=2)\n\n\nprincipal = pca.fit_transform(df)\npdf = pd.DataFrame(principal)\npdf.columns = ['xhat', 'yhat']\npdf.head(2)\n\n\n\n\n\n\n\n\nxhat\nyhat\n\n\n\n\n0\n-0.489949\n-0.679976\n\n\n1\n-0.519099\n0.544827"
  },
  {
    "objectID": "posts/clustering/Clustering.html#elbow-criterion",
    "href": "posts/clustering/Clustering.html#elbow-criterion",
    "title": "Clustering ##",
    "section": "Elbow Criterion",
    "text": "Elbow Criterion\n\nd = {}\nfor k in range(1, 6):\n    kmeans = KMeans(n_init=10, n_clusters=k, max_iter=1000).fit(pdf)\n    d[k] = kmeans.inertia_\n\n\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.plot(list(d.keys()), list(d.values()))\nplt.xlabel(\"N Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()\n\n\n\n\nWe can see the SSE (Sum of Squared Errors) is decreasing as clusters increase.\n\nkm = KMeans(n_init=10, n_clusters=3)\nkm.fit(principal)\n\nKMeans(n_clusters=3, n_init=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3, n_init=10)\n\n\n\nlabels = km.predict(pdf)\n\nC:\\Users\\Kaustubh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but KMeans was fitted without feature names\n  warnings.warn(\n\n\n\nplt.scatter(pdf['xhat'], pdf['yhat'], c=KMeans(n_clusters=3).fit_predict(pdf))\ncentroids = km.cluster_centers_\ncentroidXHat = centroids[:,0]\ncentroidYHat = centroids[:,1]\nplt.scatter(centroidXHat, centroidYHat, marker='X',s=10, color='r')\n\nC:\\Users\\Kaustubh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n&lt;matplotlib.collections.PathCollection at 0x226e80e9a10&gt;"
  },
  {
    "objectID": "posts/clustering/Clustering.html#results",
    "href": "posts/clustering/Clustering.html#results",
    "title": "Clustering ##",
    "section": "Results",
    "text": "Results\nAs you can see, the algorithm created three clusters of the datapoints along with their centroids. Of course, the data is displayed in the condendensed PCA format. After converting back to regular expanded format we should be able to see the corresponding datapoints and their characteristics."
  },
  {
    "objectID": "posts/clustering/Clustering.html#hierarchical-clustering",
    "href": "posts/clustering/Clustering.html#hierarchical-clustering",
    "title": "Clustering ##",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nimport scipy.cluster.hierarchy as shc\n\n\nagg = AgglomerativeClustering(n_clusters=3)\nagg.fit(pdf)\n\nAgglomerativeClustering(n_clusters=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AgglomerativeClusteringAgglomerativeClustering(n_clusters=3)\n\n\n\nplt.scatter(pdf['xhat'], pdf['yhat'], c=AgglomerativeClustering(n_clusters=3).fit_predict(pdf))\n\n&lt;matplotlib.collections.PathCollection at 0x226ee4c01d0&gt;\n\n\n\n\n\nAs you can see, we got similar results from Agglomerative clustering however slightly different. We can see there’s more overlap between datapoints here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mlblog",
    "section": "",
    "text": "Clustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nKaustubh Kale\n\n\n\n\n\n\n  \n\n\n\n\nNonlinear Regression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nKaustubh Kale\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2023\n\n\nKaustubh Kale\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nKaustubh Kale\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Kaustubh, an MEng student at Virginia Tech and this is my ML Blog."
  },
  {
    "objectID": "posts/classification/Classification.html",
    "href": "posts/classification/Classification.html",
    "title": "What is Classification?",
    "section": "",
    "text": "Classification is assigning some label to an input value based on some predictor variables. These labels are predefined and predictions are made based on previous predictions on similar data. There are various classifcation techniques such as Naive Bayes, K-Nearest-Neighbors, Logistic Regression, SVMs, etc. The most simple form of classification is Binary Classification, where one of two labels can be chosen from. Some use cases are image classification, spam filtering, or even medical diagnosis. One tough part with classification is especially with supervised learning techniques, the data has to be labeled into the categories which requires manual effort.\n\nSMS Spam Dataset\nWe will use an SMS Spam dataset from Kaggle to determine if a message is spam or not. We will use different techniques to determine whether there is spam in a message.\n\nimport pandas as pd\ndf = pd.read_csv('spam.csv', encoding='ISO-8859-1')\ndf.describe()\n\n\n\n\n\n\n\n\nv1\nv2\nUnnamed: 2\nUnnamed: 3\nUnnamed: 4\n\n\n\n\ncount\n5572\n5572\n50\n12\n6\n\n\nunique\n2\n5169\n43\n10\n5\n\n\ntop\nham\nSorry, I'll call later\nbt not his girlfrnd... G o o d n i g h t . . .@\"\nMK17 92H. 450Ppw 16\"\nGNT:-)\"\n\n\nfreq\n4825\n30\n3\n2\n2\n\n\n\n\n\n\n\n\n\nCleaning\nThis dataset needs a lot of cleaning. Ham: 0, Spam: 1.\n\ndf = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndf = df.rename(columns={\"v1\":\"label\", \"v2\":\"sms\"})\ndf['label'] = df['label'].map({'ham':0, 'spam':1})\ndf.head()\n\n\n\n\n\n\n\n\nlabel\nsms\n\n\n\n\n0\n0\nGo until jurong point, crazy.. Available only ...\n\n\n1\n0\nOk lar... Joking wif u oni...\n\n\n2\n1\nFree entry in 2 a wkly comp to win FA Cup fina...\n\n\n3\n0\nU dun say so early hor... U c already then say...\n\n\n4\n0\nNah I don't think he goes to usf, he lives aro...\n\n\n\n\n\n\n\n\ndf.drop_duplicates(inplace=True)\n\nVisualization the lengths of the messages and different types of words can help us determine their correct classification\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf[\"len\"] = df[\"sms\"].apply(len)\ndf['len'].plot(bins=50, kind='hist')\nplt.ylabel(\"Count\")\nplt.ylabel(\"Num chars\")\nplt.show()\n\n\n\n\n\ndf.hist(column='len', by='label', bins=50)\n\narray([&lt;Axes: title={'center': '0'}&gt;, &lt;Axes: title={'center': '1'}&gt;],\n      dtype=object)\n\n\n\n\n\n\nfrom wordcloud import WordCloud\nwc = WordCloud()\nhamWords = wc.generate(df[df['label'] == 0]['sms'].str.cat(sep=\" \"))\nplt.imshow(hamWords)\n\n&lt;matplotlib.image.AxesImage at 0x295a5656810&gt;\n\n\n\n\n\n\nspamWords = wc.generate(df[df['label'] == 1]['sms'].str.cat(sep=\" \"))\nplt.imshow(spamWords)\n\n&lt;matplotlib.image.AxesImage at 0x295a5672610&gt;\n\n\n\n\n\n\n\nPreprocessing\nPreprocessing is a huge process before text classification. We need to figure out a way to remove common words (stopwords) so they don’t affect our model. We also need to develop some system for our classification purposes. NLTK library is commonly used for this.\n\nimport nltk\nimport re\nnltk.download(\"stopwords\")   \nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps=PorterStemmer()\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Kaustubh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\ncorpus = []\nlabels = []\nfor i in range(0, len(df)):\n    try:\n        cleaned = re.sub('^[a-zA-Z]',' ', df['sms'][i])\n        cleaned = cleaned.lower()\n        cleaned.split()\n        cleaned=[ps.stem(cleaned) for word in cleaned if not word in stopwords.words('english')]\n        cleaned = ''.join(cleaned)\n        corpus.append(cleaned)\n        labels.append(df['label'][i])\n    except KeyError as err:\n        continue\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer()\nX=cv.fit_transform(corpus).toarray()\nY=labels\nX\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)\n\n\n\nfrom sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.2)\n\n\n\nNaive Bayes Method\nThis method uses Bayes Theorem and considers that features are independent of each other. It will consider the individual words in an sms independent and determine the probability of spam/ham using the classic Bayes Theorem formula.\n\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(xTrain, yTrain)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\nyPred = nb.predict(xTest)\nfrom sklearn.metrics import classification_report\nprint(classification_report(yTest, yPred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.91      0.94       849\n           1       0.57      0.88      0.69       115\n\n    accuracy                           0.91       964\n   macro avg       0.77      0.89      0.82       964\nweighted avg       0.93      0.91      0.91       964\n\n\n\nWe can see Naive Bayes gave us a 0.91 accuracy score\n\n\nLogistic Regression Classification\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter=500)\nlr.fit(xTrain, yTrain)\n\nLogisticRegression(max_iter=500)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=500)\n\n\n\nyPred = lr.predict(xTest)\nprint(classification_report(yTest, yPred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.99       849\n           1       0.94      0.86      0.90       115\n\n    accuracy                           0.98       964\n   macro avg       0.96      0.93      0.94       964\nweighted avg       0.98      0.98      0.98       964\n\n\n\nWe got a slightly higher accuracy with the LogisticRegression model\n\n\nSVM Classifier\nThis classifier works by setting a hyperplane of some features and dividng the data into two sides of the hyperplane. One side will be classified as spam and the other as ham. SVM is computationally expensive but works well on smaller datasets.\n\nfrom sklearn import svm\nsvc = svm.SVC()\nsvc.fit(xTrain, yTrain)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC()\n\n\n\nyPred = svc.predict(xTest)\nprint(classification_report(yTest, yPred))\n\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.99       849\n           1       0.99      0.81      0.89       115\n\n    accuracy                           0.98       964\n   macro avg       0.98      0.90      0.94       964\nweighted avg       0.98      0.98      0.98       964\n\n\n\nThe SVM classifier also gave a good accuracy"
  },
  {
    "objectID": "posts/linear-regression/LinearRegression.html",
    "href": "posts/linear-regression/LinearRegression.html",
    "title": "What is Linear Regression?",
    "section": "",
    "text": "Regression is a model that uses variables to predict the outcomes of other variables. Linear regression assumes the variables have a linear relation, and hence tries to predict outcomes linearly. Some applications of linear regression can include predicting survival rate, political affiliation, or even insurance costs. There are two types of linear regression, simple and multi, which only differ by the number of predictor variables.\n\nExample Dataset\nFor this linear regression example we will use a simple salary prediction dataset from Kaggle. The data set correlates years of experience with salary, and we can use simple linear regression to predict salary based on experience. If there were more predictors other than years of experience in the dataset we would use multilinear regression, but the underyling concept is the same - fitting predictors across a line plot to predict a variable\n\nimport pandas as pd\ndf = pd.read_csv('Salary_dataset.csv')\ndf.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYearsExperience\nSalary\n\n\n\n\ncount\n30.000000\n30.000000\n30.000000\n\n\nmean\n14.500000\n5.413333\n76004.000000\n\n\nstd\n8.803408\n2.837888\n27414.429785\n\n\nmin\n0.000000\n1.200000\n37732.000000\n\n\n25%\n7.250000\n3.300000\n56721.750000\n\n\n50%\n14.500000\n4.800000\n65238.000000\n\n\n75%\n21.750000\n7.800000\n100545.750000\n\n\nmax\n29.000000\n10.600000\n122392.000000\n\n\n\n\n\n\n\n\n\nPreprocessing\nWe need to do some dataset cleaning to easily feed it into the linear regression model and do visualizations. This dataset has an extra column and could have duplicate/null values.\n\ndf = df.drop('Unnamed: 0', axis=1)\ndf.describe()\n\n\n\n\n\n\n\n\nYearsExperience\nSalary\n\n\n\n\ncount\n30.000000\n30.000000\n\n\nmean\n5.413333\n76004.000000\n\n\nstd\n2.837888\n27414.429785\n\n\nmin\n1.200000\n37732.000000\n\n\n25%\n3.300000\n56721.750000\n\n\n50%\n4.800000\n65238.000000\n\n\n75%\n7.800000\n100545.750000\n\n\nmax\n10.600000\n122392.000000\n\n\n\n\n\n\n\n\ndf.isnull().sum(), df.duplicated().sum()\n\n(YearsExperience    0\n Salary             0\n dtype: int64,\n 0)\n\n\n\n\nAssumptions\nThere are a few assumptions before we can properly use linear regression on a dataset. To validate those assumptions exist we need to do some visualization on the dataset.\n\nResidual Independence: There shouldn’t be correlation between residual terms. We can see from the first visualization there is no such correlation.\nResidual Linearity: We can see that there is a linear relationship between the independent and dependent variable. We can safely use linear regression to assume a line of best fit for prediction.\nNormal Distribution: While not a perfect normal distribution, the residuals do form close to a normal distribution.\nEqual Variance: The residuals have close to equal variance and there aren’t too many outliers or extreme values.\n\nWe can see from the assumptions listed above that the datset does pretty much conform to these assumptions and we can use linear regression to predict salary based on years of experience. One shortcoming of this dataset is the size, however, it would be nice to have more data points to additionally confirm these assumptions. Nevertheless, we can safely carry out linear regression.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nsns.residplot(df, x='YearsExperience', y='Salary')\n\n&lt;Axes: xlabel='YearsExperience', ylabel='Salary'&gt;\n\n\n\n\n\n\nsns.regplot(df, x='YearsExperience', y='Salary')\n\n&lt;Axes: xlabel='YearsExperience', ylabel='Salary'&gt;\n\n\n\n\n\n\nsns.displot(df, x=\"YearsExperience\", kde=True)\n\n\n\n\n\nsns.displot(df, x=\"Salary\", kde=True)\n\n\n\n\n\nsns.heatmap(df.corr(), annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nLinear Regression\nWe will perform linear regression by first splitting the data into train/test. We will use an 80/20 split for train/test data.\n\nfrom sklearn.model_selection import train_test_split\nX=df['YearsExperience']\nY=df['Salary']\nxTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.2)\nlen(xTrain), len(yTrain)\n\n(24, 24)\n\n\nWe will use scikit-learn’s built in linear regression model to make our predictions\n\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nlr = LinearRegression()\nxTrain = np.array(xTrain).reshape(-1, 1)\nyTrain = np.array(yTrain).reshape(-1, 1)\nlr.fit(xTrain, yTrain)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nxTest = np.array(xTest).reshape(-1, 1)\nlr.predict(xTest)\n\narray([[125736.3007223 ],\n       [ 75827.51046153],\n       [ 55672.0374716 ],\n       [117098.24086948],\n       [123816.73186612],\n       [ 93103.63016718]])\n\n\nWe can see our linear regression line closely resembles our train data, albeit with a little bit of variance, which is to be expected.\n\nplt.scatter(df['YearsExperience'], df['Salary'])\nplt.plot(xTest, lr.predict(xTest),color='g')\n\n\n\n\n\n\nCalculating Accuracy\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nyPred = lr.predict(xTest)\nrmse = np.sqrt(mean_squared_error(yTest, yPred))\nr2 = r2_score(yTest, yPred)\n\nrmse, r2\n\n(4431.5473096452315, 0.9662834154887496)\n\n\nWe can see that linear regression can predict closely with about 92.7% accuracy as well as a variance of $7289 with salary amount. Perhaps the results would be stronger with a larger dataset or other factors. However, for the purposes of simple linear regression, this example is sufficient."
  }
]