[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/nonlinear-regression/NonlinearRegression.html",
    "href": "posts/nonlinear-regression/NonlinearRegression.html",
    "title": "What is Nonlinear Regression?",
    "section": "",
    "text": "Nonlinear, like linear regression tries to model a relation between independent and dependent variables using a function. However, linear regression follows a traditional y=mx+b line graph, while nonlinear relations can have more complex functions of different orders. Some example functions are quadratic, exponential, logistic, logarithmic, etc. To find the fitness of the nonlinear regression model, a sum of squares is used (calculating the difference of the mean of the data and each point). Use cases for nonlinear regression consist of any time of nonlinear data, such as population growth over time, stock market forecasting, etc.\n\nSome types of Nonlinear Regression\n\n\nQuadratic\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef quadratic(x, a, b, c):\n    return a * x**2 + b * x + c\n\nx = np.linspace(-10, 10, 100)\n\na = 1\nb = 2\nc = 1\n\ny = quadratic(x, a, b, c)\n\nnum_points = 50\nrandX = np.random.uniform(-8, 8, num_points)\nrandY = quadratic(randX, a, b, c) + np.random.normal(0, 5, num_points)\n\nplt.plot(x, y)\nplt.scatter(randX, randY, color='red', label='Random Data Points')\n\nplt.title('Quadratic Function with Scatter Plot')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(0, color='k', linewidth=0.5)\nplt.axvline(0, color='k', linewidth=0.5)\nplt.grid(color='y', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n\nExponential\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef exponential(x, a, b):\n    return a * np.exp(b * x)\n\nx = np.linspace(-10, 10, 100)\n\na = 2\nb = 1\n\ny = exponential(x, a, b)\n\nnum_points = 50\nrandX = np.random.uniform(-8, 8, num_points)\nrandY = exponential(randX, a, b) + np.random.normal(0, 5, num_points)\n\nplt.plot(x, y)\nplt.scatter(randX, randY, color='red', label='Random Data Points')\n\nplt.title('Exponential Function with Scatter Plot')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(0, color='k', linewidth=0.5)\nplt.axvline(0, color='k', linewidth=0.5)\nplt.grid(color='y', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n\nLogistic\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef logistic(x, a, b):\n    return 1 / (1 +np.exp(-a*(x-b)))\n\nx = np.linspace(-10, 10, 100)\n\na = 1\nb = 2\n\ny = logistic(x, a, b)\n\nnum_points = 50\nrandX = np.random.uniform(-8, 8, num_points)\nrandY = logistic(randX, a, b) + np.random.normal(0, 5, num_points)\n\nplt.plot(x, y)\nplt.scatter(randX, randY, color='red', label='Random Data Points')\n\nplt.title('Logistic Function with Scatter Plot')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(0, color='k', linewidth=0.5)\nplt.axvline(0, color='k', linewidth=0.5)\nplt.grid(color='y', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef logarithmic(x, a, b):\n    return a * np.log(x) + b\n\nx = np.linspace(0.1, 10, 100)\n\na = 2\nb = 1\n\ny = logarithmic(x, a, b)\n\nnum_points = 50\nrandX = np.random.uniform(0.1, 8, num_points)\nrandY = logarithmic(randX, a, b) + np.random.normal(0, 5, num_points)\n\nplt.plot(x, y)\nplt.scatter(randX, randY, color='red', label='Random Data Points')\n\nplt.title('Logarithmic Function with Scatter Plot')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.axhline(0, color='k', linewidth=0.5)\nplt.axvline(0, color='k', linewidth=0.5)\nplt.grid(color='y', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n\nWorld GDP Dataset\nFor this nonlinear regression analysis we will use a dataset of World GDP over time. The data just shows how World GDP changed every year and we can use some form of nonlinear regression to predict how it will be in the future. Nonlinear regression is great for time series data. This dataset is publicly available.\n\nimport pandas as pd\ndf = pd.read_csv('Worldgdp.csv')\ndf.describe()\n\n\n\n\n\n\n\n\nYear\nGDP\n\n\n\n\ncount\n66.000000\n6.600000e+01\n\n\nmean\n1982.500000\n4.328561e+13\n\n\nstd\n19.196354\n2.776368e+13\n\n\nmin\n1950.000000\n9.250000e+12\n\n\n25%\n1966.250000\n1.987500e+13\n\n\n50%\n1982.500000\n3.630000e+13\n\n\n75%\n1998.750000\n5.970000e+13\n\n\nmax\n2015.000000\n1.080000e+14\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.regplot(df, x='Year', y='GDP')\n\n&lt;Axes: xlabel='Year', ylabel='GDP'&gt;\n\n\n\n\n\nAs we can see, linear regression doesn’t show a good prediction of future GDP so we need to use one of the nonlinear regression functions. Just intuitively from the graph, we can see that a logistic function will probably fit the function closely. We can use scipy to optimize the function.\ndef logistic(x, a, b): return 1 / (1 +np.exp(-a*(x-b)))\nY_pred = logistic(df[‘Year’], 0.1 , 2000) plt.plot(df[‘Year’], Y_pred*1e14) plt.plot(df[‘Year’], df[‘GDP’], ‘g.’)\n\nfrom scipy.optimize import curve_fit\nxVals = df['Year'] / max(df['Year'])\nyVals = df['GDP'] / max(df['GDP'])\n(a, b), _ = curve_fit(logistic, xVals, yVals)\n\n\nx = np.linspace(min(df['Year']), max(df['Year']))\nx /= max(x)\ny = logistic(x, a, b)\nplt.plot(xVals* max(df['Year']), yVals*max(df['GDP']), 'g.')\nplt.plot(x*max(df['Year']),y*max(df['GDP']), linewidth=2.0)\nplt.ylabel('World GDP')\nplt.xlabel('Year')\nplt.show()\n\n\n\n\nWe can intuitively see that scipy managed to fit the logistic function appropriately with our data and it resembles pretty closely\nLets predict world GDP in 2030 using the nonlinear regression model\n\n(logistic([2035/max(df['Year'])], a, b) * 1e14)[0]\n\n94677239359468.88"
  },
  {
    "objectID": "posts/classification/Classification.html",
    "href": "posts/classification/Classification.html",
    "title": "What is Classification?",
    "section": "",
    "text": "Classification is assigning some label to an input value based on some predictor variables. These labels are predefined and predictions are made based on previous predictions on similar data. There are various classifcation techniques such as Naive Bayes, K-Nearest-Neighbors, Logistic Regression, SVMs, etc. The most simple form of classification is Binary Classification, where one of two labels can be chosen from. Some use cases are image classification, spam filtering, or even medical diagnosis. One tough part with classification is especially with supervised learning techniques, the data has to be labeled into the categories which requires manual effort.\n\nSMS Spam Dataset\nWe will use an SMS Spam dataset from Kaggle to determine if a message is spam or not. We will use different techniques to determine whether there is spam in a message.\n\nimport pandas as pd\ndf = pd.read_csv('spam.csv', encoding='ISO-8859-1')\ndf.describe()\n\n\n\n\n\n\n\n\nv1\nv2\nUnnamed: 2\nUnnamed: 3\nUnnamed: 4\n\n\n\n\ncount\n5572\n5572\n50\n12\n6\n\n\nunique\n2\n5169\n43\n10\n5\n\n\ntop\nham\nSorry, I'll call later\nbt not his girlfrnd... G o o d n i g h t . . .@\"\nMK17 92H. 450Ppw 16\"\nGNT:-)\"\n\n\nfreq\n4825\n30\n3\n2\n2\n\n\n\n\n\n\n\n\n\nCleaning\nThis dataset needs a lot of cleaning. Ham: 0, Spam: 1.\n\ndf = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndf = df.rename(columns={\"v1\":\"label\", \"v2\":\"sms\"})\ndf['label'] = df['label'].map({'ham':0, 'spam':1})\ndf.head()\n\n\n\n\n\n\n\n\nlabel\nsms\n\n\n\n\n0\n0\nGo until jurong point, crazy.. Available only ...\n\n\n1\n0\nOk lar... Joking wif u oni...\n\n\n2\n1\nFree entry in 2 a wkly comp to win FA Cup fina...\n\n\n3\n0\nU dun say so early hor... U c already then say...\n\n\n4\n0\nNah I don't think he goes to usf, he lives aro...\n\n\n\n\n\n\n\n\ndf.drop_duplicates(inplace=True)\n\nVisualization the lengths of the messages and different types of words can help us determine their correct classification\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf[\"len\"] = df[\"sms\"].apply(len)\ndf['len'].plot(bins=50, kind='hist')\nplt.ylabel(\"Count\")\nplt.ylabel(\"Num chars\")\nplt.show()\n\n\n\n\n\ndf.hist(column='len', by='label', bins=50)\n\narray([&lt;Axes: title={'center': '0'}&gt;, &lt;Axes: title={'center': '1'}&gt;],\n      dtype=object)\n\n\n\n\n\n\nfrom wordcloud import WordCloud\nwc = WordCloud()\nhamWords = wc.generate(df[df['label'] == 0]['sms'].str.cat(sep=\" \"))\nplt.imshow(hamWords)\n\n&lt;matplotlib.image.AxesImage at 0x295a5656810&gt;\n\n\n\n\n\n\nspamWords = wc.generate(df[df['label'] == 1]['sms'].str.cat(sep=\" \"))\nplt.imshow(spamWords)\n\n&lt;matplotlib.image.AxesImage at 0x295a5672610&gt;\n\n\n\n\n\n\n\nPreprocessing\nPreprocessing is a huge process before text classification. We need to figure out a way to remove common words (stopwords) so they don’t affect our model. We also need to develop some system for our classification purposes. NLTK library is commonly used for this.\n\nimport nltk\nimport re\nnltk.download(\"stopwords\")   \nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps=PorterStemmer()\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Kaustubh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\ncorpus = []\nlabels = []\nfor i in range(0, len(df)):\n    try:\n        cleaned = re.sub('^[a-zA-Z]',' ', df['sms'][i])\n        cleaned = cleaned.lower()\n        cleaned.split()\n        cleaned=[ps.stem(cleaned) for word in cleaned if not word in stopwords.words('english')]\n        cleaned = ''.join(cleaned)\n        corpus.append(cleaned)\n        labels.append(df['label'][i])\n    except KeyError as err:\n        continue\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer()\nX=cv.fit_transform(corpus).toarray()\nY=labels\nX\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)\n\n\n\nfrom sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.2)\n\n\n\nNaive Bayes Method\nThis method uses Bayes Theorem and considers that features are independent of each other. It will consider the individual words in an sms independent and determine the probability of spam/ham using the classic Bayes Theorem formula.\n\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(xTrain, yTrain)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\nyPred = nb.predict(xTest)\nfrom sklearn.metrics import classification_report\nprint(classification_report(yTest, yPred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.91      0.94       849\n           1       0.57      0.88      0.69       115\n\n    accuracy                           0.91       964\n   macro avg       0.77      0.89      0.82       964\nweighted avg       0.93      0.91      0.91       964\n\n\n\nWe can see Naive Bayes gave us a 0.91 accuracy score\n\n\nLogistic Regression Classification\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter=500)\nlr.fit(xTrain, yTrain)\n\nLogisticRegression(max_iter=500)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=500)\n\n\n\nyPred = lr.predict(xTest)\nprint(classification_report(yTest, yPred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.99       849\n           1       0.94      0.86      0.90       115\n\n    accuracy                           0.98       964\n   macro avg       0.96      0.93      0.94       964\nweighted avg       0.98      0.98      0.98       964\n\n\n\nWe got a slightly higher accuracy with the LogisticRegression model\n\n\nSVM Classifier\nThis classifier works by setting a hyperplane of some features and dividng the data into two sides of the hyperplane. One side will be classified as spam and the other as ham. SVM is computationally expensive but works well on smaller datasets.\n\nfrom sklearn import svm\nsvc = svm.SVC()\nsvc.fit(xTrain, yTrain)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC()\n\n\n\nyPred = svc.predict(xTest)\nprint(classification_report(yTest, yPred))\n\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.99       849\n           1       0.99      0.81      0.89       115\n\n    accuracy                           0.98       964\n   macro avg       0.98      0.90      0.94       964\nweighted avg       0.98      0.98      0.98       964\n\n\n\nThe SVM classifier also gave a good accuracy"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mlblog",
    "section": "",
    "text": "What is Classification?\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWhat is Linear Regression?\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWhat is Nonlinear Regression?\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/linear-regression/LinearRegression.html",
    "href": "posts/linear-regression/LinearRegression.html",
    "title": "What is Linear Regression?",
    "section": "",
    "text": "Regression is a model that uses variables to predict the outcomes of other variables. Linear regression assumes the variables have a linear relation, and hence tries to predict outcomes linearly. Some applications of linear regression can include predicting survival rate, political affiliation, or even insurance costs. There are two types of linear regression, simple and multi, which only differ by the number of predictor variables.\n\nExample Dataset\nFor this linear regression example we will use a simple salary prediction dataset from Kaggle. The data set correlates years of experience with salary, and we can use simple linear regression to predict salary based on experience. If there were more predictors other than years of experience in the dataset we would use multilinear regression, but the underyling concept is the same - fitting predictors across a line plot to predict a variable\n\nimport pandas as pd\ndf = pd.read_csv('Salary_dataset.csv')\ndf.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYearsExperience\nSalary\n\n\n\n\ncount\n30.000000\n30.000000\n30.000000\n\n\nmean\n14.500000\n5.413333\n76004.000000\n\n\nstd\n8.803408\n2.837888\n27414.429785\n\n\nmin\n0.000000\n1.200000\n37732.000000\n\n\n25%\n7.250000\n3.300000\n56721.750000\n\n\n50%\n14.500000\n4.800000\n65238.000000\n\n\n75%\n21.750000\n7.800000\n100545.750000\n\n\nmax\n29.000000\n10.600000\n122392.000000\n\n\n\n\n\n\n\n\n\nPreprocessing\nWe need to do some dataset cleaning to easily feed it into the linear regression model and do visualizations. This dataset has an extra column and could have duplicate/null values.\n\ndf = df.drop('Unnamed: 0', axis=1)\ndf.describe()\n\n\n\n\n\n\n\n\nYearsExperience\nSalary\n\n\n\n\ncount\n30.000000\n30.000000\n\n\nmean\n5.413333\n76004.000000\n\n\nstd\n2.837888\n27414.429785\n\n\nmin\n1.200000\n37732.000000\n\n\n25%\n3.300000\n56721.750000\n\n\n50%\n4.800000\n65238.000000\n\n\n75%\n7.800000\n100545.750000\n\n\nmax\n10.600000\n122392.000000\n\n\n\n\n\n\n\n\ndf.isnull().sum(), df.duplicated().sum()\n\n(YearsExperience    0\n Salary             0\n dtype: int64,\n 0)\n\n\n\n\nAssumptions\nThere are a few assumptions before we can properly use linear regression on a dataset. To validate those assumptions exist we need to do some visualization on the dataset.\n\nResidual Independence: There shouldn’t be correlation between residual terms. We can see from the first visualization there is no such correlation.\nResidual Linearity: We can see that there is a linear relationship between the independent and dependent variable. We can safely use linear regression to assume a line of best fit for prediction.\nNormal Distribution: While not a perfect normal distribution, the residuals do form close to a normal distribution.\nEqual Variance: The residuals have close to equal variance and there aren’t too many outliers or extreme values.\n\nWe can see from the assumptions listed above that the datset does pretty much conform to these assumptions and we can use linear regression to predict salary based on years of experience. One shortcoming of this dataset is the size, however, it would be nice to have more data points to additionally confirm these assumptions. Nevertheless, we can safely carry out linear regression.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nsns.residplot(df, x='YearsExperience', y='Salary')\n\n&lt;Axes: xlabel='YearsExperience', ylabel='Salary'&gt;\n\n\n\n\n\n\nsns.regplot(df, x='YearsExperience', y='Salary')\n\n&lt;Axes: xlabel='YearsExperience', ylabel='Salary'&gt;\n\n\n\n\n\n\nsns.displot(df, x=\"YearsExperience\", kde=True)\n\n\n\n\n\nsns.displot(df, x=\"Salary\", kde=True)\n\n\n\n\n\nsns.heatmap(df.corr(), annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nLinear Regression\nWe will perform linear regression by first splitting the data into train/test. We will use an 80/20 split for train/test data.\n\nfrom sklearn.model_selection import train_test_split\nX=df['YearsExperience']\nY=df['Salary']\nxTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.2)\nlen(xTrain), len(yTrain)\n\n(24, 24)\n\n\nWe will use scikit-learn’s built in linear regression model to make our predictions\n\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nlr = LinearRegression()\nxTrain = np.array(xTrain).reshape(-1, 1)\nyTrain = np.array(yTrain).reshape(-1, 1)\nlr.fit(xTrain, yTrain)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nxTest = np.array(xTest).reshape(-1, 1)\nlr.predict(xTest)\n\narray([[125736.3007223 ],\n       [ 75827.51046153],\n       [ 55672.0374716 ],\n       [117098.24086948],\n       [123816.73186612],\n       [ 93103.63016718]])\n\n\nWe can see our linear regression line closely resembles our train data, albeit with a little bit of variance, which is to be expected.\n\nplt.scatter(df['YearsExperience'], df['Salary'])\nplt.plot(xTest, lr.predict(xTest),color='g')\n\n\n\n\n\n\nCalculating Accuracy\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nyPred = lr.predict(xTest)\nrmse = np.sqrt(mean_squared_error(yTest, yPred))\nr2 = r2_score(yTest, yPred)\n\nrmse, r2\n\n(4431.5473096452315, 0.9662834154887496)\n\n\nWe can see that linear regression can predict closely with about 92.7% accuracy as well as a variance of $7289 with salary amount. Perhaps the results would be stronger with a larger dataset or other factors. However, for the purposes of simple linear regression, this example is sufficient."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]