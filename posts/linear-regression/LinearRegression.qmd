---
title: "Linear Regression"
author: "Kaustubh Kale"
date: "2023-11-26"
categories: [news, code, analysis]
jupyter: python3
---


Regression is a model that uses variables to predict the outcomes of other variables. Linear regression assumes the variables have a linear relation, and hence tries to predict outcomes linearly. Some applications of linear regression can include predicting survival rate, political affiliation, or even insurance costs. There are two types of linear regression, simple and multi, which only differ by the number of predictor variables. 


### Example Dataset ###
For this linear regression example we will use a simple salary prediction dataset from Kaggle. The data set correlates years of experience with salary, and we can use simple linear regression to predict salary based on experience. If there were more predictors other than years of experience in the dataset we would use multilinear regression, but the underyling concept is the same - fitting predictors across a line plot to predict a variable

```{python}
import pandas as pd
df = pd.read_csv('Salary_dataset.csv')
df.describe()
```

### Preprocessing ###
We need to do some dataset cleaning to easily feed it into the linear regression model and do visualizations. This dataset has an extra column and could have duplicate/null values.

```{python}
df = df.drop('Unnamed: 0', axis=1)
df.describe()
```

```{python}
df.isnull().sum(), df.duplicated().sum()
```

### Assumptions ###
There are a few assumptions before we can properly use linear regression on a dataset. To validate those assumptions exist we need to do some visualization on the dataset.

1. **Residual Independence**: There shouldn't be correlation between residual terms. We can see from the first visualization there is no such correlation.
2. **Residual Linearity**: We can see that there is a linear relationship between the independent and dependent variable. We can safely use linear regression to assume a line of best fit for prediction.
3. **Normal Distribution**: While not a perfect normal distribution, the residuals do form close to a normal distribution.
4. **Equal Variance**: The residuals have close to equal variance and there aren't too many outliers or extreme values.

We can see from the assumptions listed above that the datset does pretty much conform to these assumptions and we can use linear regression to predict salary based on years of experience. One shortcoming of this dataset is the size, however, it would be nice to have more data points to additionally confirm these assumptions. Nevertheless, we can safely carry out linear regression.

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
```

```{python}
sns.residplot(df, x='YearsExperience', y='Salary')
```

```{python}
sns.regplot(df, x='YearsExperience', y='Salary')
```

```{python}
sns.displot(df, x="YearsExperience", kde=True)
```

```{python}
sns.displot(df, x="Salary", kde=True)
```

```{python}
sns.heatmap(df.corr(), annot=True)
```

### Linear Regression ###
We will perform linear regression by first splitting the data into train/test. We will use an 80/20 split for train/test data.

```{python}
from sklearn.model_selection import train_test_split
X=df['YearsExperience']
Y=df['Salary']
xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.2)
len(xTrain), len(yTrain)
```

We will use scikit-learn's built in linear regression model to make our predictions

```{python}
from sklearn.linear_model import LinearRegression
import numpy as np
lr = LinearRegression()
xTrain = np.array(xTrain).reshape(-1, 1)
yTrain = np.array(yTrain).reshape(-1, 1)
lr.fit(xTrain, yTrain)
```

```{python}
xTest = np.array(xTest).reshape(-1, 1)
lr.predict(xTest)
```

We can see our linear regression line closely resembles our train data, albeit with a little bit of variance, which is to be expected.

```{python}
plt.scatter(df['YearsExperience'], df['Salary'])
plt.plot(xTest, lr.predict(xTest),color='g')
```

### Calculating Accuracy ###

```{python}
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
yPred = lr.predict(xTest)
rmse = np.sqrt(mean_squared_error(yTest, yPred))
r2 = r2_score(yTest, yPred)

rmse, r2
```

We can see that linear regression can predict closely with about 92.7% accuracy as well as a variance of $7289 with salary amount. Perhaps the results would be stronger with a larger dataset or other factors. However, for the purposes of simple linear regression, this example is sufficient.


